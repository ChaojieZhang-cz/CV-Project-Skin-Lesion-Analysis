{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import scipy.misc\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from unet import UNet\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([])\n",
    "train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform.transforms.append(transforms.RandomVerticalFlip())\n",
    "transforms.RandomRotation(180, expand=True)\n",
    "train_transform.transforms.append(transforms.RandomResizedCrop(572, scale=(0.08, 1.0), ratio=(1.0,1.0)))\n",
    "train_transform.transforms.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0, hue=0))\n",
    "train_transform.transforms.append(transforms.ToTensor())\n",
    "train_transform.transforms.append(transforms.Normalize([0.4373, 0.4434, 0.4725],[0.1201, 0.1231, 0.1052]))\n",
    "train_transform.transforms.append(transforms.RandomErasing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([])\n",
    "test_transform.transforms.append(transforms.RandomResizedCrop(572, scale=(1.0, 1.0), ratio=(1.0,1.0)))\n",
    "test_transform.transforms.append(transforms.ToTensor())\n",
    "test_transform.transforms.append(transforms.Normalize([0.4373, 0.4434, 0.4725],[0.1201, 0.1231, 0.1052]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_dataset(Dataset):\n",
    "    def __init__(self, df_path, train = False):\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image_name = self.df.iloc[idx]['image']\n",
    "        image_path = '/scratch/cz2064/myjupyter/Computer Vision/Project/Data/classification/images_preprocessed/'\\\n",
    "        + image_name + '.jpg'\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.train:\n",
    "            image_tensor = train_transform(image)\n",
    "        else:\n",
    "            image_tensor = test_transform(image)\n",
    "            \n",
    "        metadata = self.df.loc[idx][['sex_index','age_index','anterior torso', 'head/neck', 'lateral torso',\\\n",
    "                                     'lower extremity','oral/genital', 'palms/soles', 'posterior torso',\\\n",
    "                                     'upper extremity']].values.astype('float')\n",
    "        metadata = torch.tensor(metadata, dtype=torch.float)\n",
    "        label = self.df.loc[idx][['MEL','NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC','UNK']].values.astype('float')\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        label = label.data.max(-1)[1]\n",
    "        sample = {'x1': image_tensor, 'x2':metadata,'y': label}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = '/scratch/cz2064/myjupyter/Computer Vision/Project/Jupyter Notebook/train.csv'\n",
    "val_df_path = '/scratch/cz2064/myjupyter/Computer Vision/Project/Jupyter Notebook/val.csv'\n",
    "test_df_path = '/scratch/cz2064/myjupyter/Computer Vision/Project/Jupyter Notebook/test.csv'\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(image_dataset(train_df_path,train = True), batch_size=BATCH_SIZE, shuffle=True,num_workers=16,pin_memory=True)\n",
    "val_loader = DataLoader(image_dataset(val_df_path), batch_size=BATCH_SIZE, shuffle=True,num_workers=16,pin_memory=True)\n",
    "test_loader = DataLoader(image_dataset(test_df_path), batch_size=BATCH_SIZE, shuffle=False,num_workers=16,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_tensor = next(iter(train_loader))['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inv_normalize = transforms.Normalize(mean=[-0.4373/0.1201, -0.4434/0.1231, -0.4725/0.1052],                                     std=[1/0.1201, 1/0.1231, 1/0.1052])\\nimage = inv_normalize(image_tensor)\\ntransforms.ToPILImage()(image)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''inv_normalize = transforms.Normalize(mean=[-0.4373/0.1201, -0.4434/0.1231, -0.4725/0.1052],\\\n",
    "                                     std=[1/0.1201, 1/0.1231, 1/0.1052])\n",
    "image = inv_normalize(image_tensor)\n",
    "transforms.ToPILImage()(image)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = sample['x1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 572, 572])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample['y'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_net = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/scratch/cz2064/myjupyter/Computer Vision/Project/Jupyter Notebook/unet/unet_carvana_scale1_epoch5.pth'\n",
    "u_net.load_state_dict(torch.load(path,map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.inc = u_net.inc\n",
    "        self.down1 = u_net.down1\n",
    "        self.down2 = u_net.down2\n",
    "        self.down3 = u_net.down3\n",
    "        self.down4 = u_net.down4\n",
    "        self.avg = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Linear(512, 9)\n",
    "        \n",
    "    def forward(self, x, data):\n",
    "        x = self.inc(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "        x = self.down4(x)\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.rand([2, 3, 572, 572]),1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader=train_loader, val_loader=val_loader, learning_rate=5e-5, num_epoch=100):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    distribution = torch.FloatTensor([0.17652477, 0.50707283, 0.13323245, 0.03348905, \\\n",
    "                                  0.10494111, 0.00894796, 0.00986907, 0.02592276, 0.]).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=distribution)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    train_loss_return = []\n",
    "    train_acc_return = []\n",
    "    val_loss_return = []\n",
    "    val_acc_return = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        # Training steps\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        model.train()\n",
    "        train_loss_list = []\n",
    "        for i, (sample) in enumerate(train_loader):\n",
    "            image = sample['x1'].to(device)\n",
    "            data = sample['x2'].to(device)\n",
    "            labels = sample['y'].to(device)\n",
    "            outputs = model(image,data)\n",
    "            pred = outputs.data.max(-1)[1]\n",
    "            predictions += list(pred.cpu().numpy())\n",
    "            truths += list(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum()\n",
    "            model.zero_grad()\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            print(loss)\n",
    "            train_loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # report performance\n",
    "        acc = (100 * correct / total)\n",
    "        train_acc_return.append(acc)\n",
    "        train_loss_every_epoch = np.average(train_loss_list)\n",
    "        train_loss_return.append(train_loss_every_epoch)\n",
    "        print('----------Epoch{:2d}/{:2d}----------'.format(epoch+1,num_epoch))\n",
    "        print('Train set | Loss: {:6.4f} | Accuracy: {:4.2f}% '.format(train_loss_every_epoch, acc))\n",
    "        \n",
    "        # Evaluate after every epochh\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        val_loss_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (sample) in enumerate(val_loader):\n",
    "                image = sample['x1'].to(device)\n",
    "                data = sample['x2'].to(device)\n",
    "                labels = sample['y'].to(device)\n",
    "                outputs = model(image,data)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss_list.append(loss.item())\n",
    "                pred = outputs.data.max(-1)[1]\n",
    "                predictions += list(pred.cpu().numpy())\n",
    "                truths += list(labels.cpu().numpy())\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum()\n",
    "            # report performance\n",
    "            acc = (100 * correct / total)\n",
    "            val_acc_return.append(acc)\n",
    "            val_loss_every_epoch = np.average(val_loss_list)\n",
    "            val_loss_return.append(val_loss_every_epoch)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_model_wts = model.state_dict()\n",
    "            save_model(model,train_loss_return,train_acc_return,val_loss_return,val_acc_return,best_model_wts)\n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "            print('Test set | Loss: {:6.4f} | Accuracy: {:4.2f}% | time elapse: {:>9}'\\\n",
    "                  .format(val_loss_every_epoch, acc,elapse))\n",
    "    return model,train_loss_return,train_acc_return,val_loss_return,val_acc_return,best_model_wts\n",
    "\n",
    "def save_model(model,train_loss_return,train_acc_return,val_loss_return,val_acc_return,best_model_wts):\n",
    "    state = {'best_model_wts':best_model_wts, 'model':model, \\\n",
    "             'train_loss':train_loss_return, 'train_acc':train_acc_return,\\\n",
    "             'val_loss':val_loss_return, 'val_acc':val_acc_return}\n",
    "    torch.save(state, 'checkpoint_UNet_classifier.pt')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.9857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.6241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.9138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.9989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.4698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.8081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.6180, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9d3046b5d0d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-35344d94f9c2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, learning_rate, num_epoch)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
